{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn_raw.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMbaICeJ4lYjh33V8sCSrnZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vanbk/Tensorflow/blob/master/cnn_raw.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fxmOsYyiiYy",
        "colab_type": "code",
        "outputId": "c4982e5f-25c1-43f5-b412-50a01b0251ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from __future__ import division, print_function, absolute_import\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Import MNIST data\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
        "\n",
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2eIFKJ2i1m3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Traning parameter\n",
        "learning_rate = 0.001\n",
        "num_steps = 1000\n",
        "batch_size = 128\n",
        "display_step = 10\n",
        "\n",
        "# Netework Parameter\n",
        "\n",
        "num_input = 784 # MNIST data input\n",
        "num_classes = 10 # Mnist total class\n",
        "dropout = 0.75 # Dropout, probability to keep units\n",
        "\n",
        "# tf Graph\n",
        "X = tf.placeholder(tf.float32, [None, num_input])\n",
        "Y = tf.placeholder(tf.float32, [None, num_classes])\n",
        "keep_prob = tf.placeholder(tf.float32) # dropout (keep probability)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYGzbJNHi1z8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create some wrappers for similarity\n",
        "\n",
        "def conv2d(x, W, b, strides = 1):\n",
        "  x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding= 'SAME')\n",
        "  x = tf.nn.bias_add(x, b)\n",
        "  return tf.nn.relu(x)\n",
        "\n",
        "def maxpool2d(x, k = 2):\n",
        "  return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides = [1, k, k, 1],\n",
        "                        padding = 'SAME')\n",
        "\n",
        "def conv_net(x, weights, biases, dropout):\n",
        "  x = tf.reshape(x, shape = [-1, 28, 28, 1])\n",
        "  # conv1\n",
        "  conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
        "  conv1 = maxpool2d(conv1, k =2)\n",
        "\n",
        "  #conv2\n",
        "  conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
        "  conv2 = maxpool2d(conv2, k = 2)\n",
        "\n",
        "  # fully connected layer\n",
        "  # Reshape conv2 to fit fc layer\n",
        "  fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
        "  fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
        "  fc1 = tf.nn.relu(fc1)\n",
        "\n",
        "  # dropout layer\n",
        "\n",
        "  fc1 = tf.nn.dropout(fc1, dropout)\n",
        "  # output, class prediction\n",
        "\n",
        "  out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
        "\n",
        "  return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9c7c736i14F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights = {\n",
        "    # filter_heigh = 5, filter_shape = 5, input_channel = 1, output_channel= 32\n",
        "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
        "    # filter_heigh = 5, filter_shape = 5, input_channel = 32, output_channel= 64\n",
        "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
        "\n",
        "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
        "    'out': tf.Variable(tf.random_normal([1024, num_classes]))\n",
        "}\n",
        "\n",
        "biases = {\n",
        "    'bc1': tf.Variable(tf.random_normal([32])),\n",
        "    'bc2': tf.Variable(tf.random_normal([64])),\n",
        "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
        "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7UONKBWziU1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Construct model\n",
        "logits = conv_net(X, weights, biases, keep_prob)\n",
        "prediction = tf.nn.softmax(logits)\n",
        "\n",
        "# loss and optimizer\n",
        "\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits,\n",
        "                                                                  labels = Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss_op)\n",
        "\n",
        "#Evaluate model\n",
        "correct_pre = tf.equal(tf.arg_max(prediction, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pre, tf.float32))\n",
        "\n",
        "# initialize \n",
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4I3OiWA2Rar",
        "colab_type": "code",
        "outputId": "911291c1-b198-4635-9cad-ae692a4eb075",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  # Init the variable\n",
        "  sess.run(init)\n",
        "  for step in range (1, num_steps + 1):\n",
        "    batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "    sess.run(optimizer, feed_dict={X: batch_x, Y: batch_y, keep_prob: dropout})\n",
        "\n",
        "    if step % display_step == 0 or step == 1:\n",
        "      train_loss, train_acc = sess.run([loss_op, accuracy], \n",
        "                  feed_dict={X: batch_x, Y: batch_y, keep_prob: 1.0})\n",
        "      \n",
        "      print (\"Step: {} , train_loss: {}, accuracy: {}\".format(step, train_loss, train_acc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step: 1 , train_loss: 48551.27734375, accuracy: 0.140625\n",
            "Step: 10 , train_loss: 20307.171875, accuracy: 0.21875\n",
            "Step: 20 , train_loss: 11193.8203125, accuracy: 0.4609375\n",
            "Step: 30 , train_loss: 6568.57958984375, accuracy: 0.6484375\n",
            "Step: 40 , train_loss: 4051.62451171875, accuracy: 0.703125\n",
            "Step: 50 , train_loss: 3457.119873046875, accuracy: 0.8203125\n",
            "Step: 60 , train_loss: 3486.64111328125, accuracy: 0.8046875\n",
            "Step: 70 , train_loss: 3667.2841796875, accuracy: 0.7890625\n",
            "Step: 80 , train_loss: 2564.18212890625, accuracy: 0.8671875\n",
            "Step: 90 , train_loss: 2275.117919921875, accuracy: 0.84375\n",
            "Step: 100 , train_loss: 1155.3125, accuracy: 0.8984375\n",
            "Step: 110 , train_loss: 1503.26171875, accuracy: 0.8984375\n",
            "Step: 120 , train_loss: 1504.989990234375, accuracy: 0.8984375\n",
            "Step: 130 , train_loss: 1521.0225830078125, accuracy: 0.90625\n",
            "Step: 140 , train_loss: 965.5655517578125, accuracy: 0.9140625\n",
            "Step: 150 , train_loss: 801.234130859375, accuracy: 0.9375\n",
            "Step: 160 , train_loss: 731.9411010742188, accuracy: 0.8984375\n",
            "Step: 170 , train_loss: 808.4513549804688, accuracy: 0.9375\n",
            "Step: 180 , train_loss: 1671.544677734375, accuracy: 0.875\n",
            "Step: 190 , train_loss: 941.6260986328125, accuracy: 0.9453125\n",
            "Step: 200 , train_loss: 534.0151977539062, accuracy: 0.9296875\n",
            "Step: 210 , train_loss: 801.5448608398438, accuracy: 0.9296875\n",
            "Step: 220 , train_loss: 1183.62060546875, accuracy: 0.890625\n",
            "Step: 230 , train_loss: 1277.9874267578125, accuracy: 0.9140625\n",
            "Step: 240 , train_loss: 802.2413940429688, accuracy: 0.90625\n",
            "Step: 250 , train_loss: 1318.017333984375, accuracy: 0.9375\n",
            "Step: 260 , train_loss: 1098.2506103515625, accuracy: 0.9296875\n",
            "Step: 270 , train_loss: 1304.5357666015625, accuracy: 0.9375\n",
            "Step: 280 , train_loss: 578.7800903320312, accuracy: 0.9453125\n",
            "Step: 290 , train_loss: 521.3558959960938, accuracy: 0.9375\n",
            "Step: 300 , train_loss: 792.3294677734375, accuracy: 0.953125\n",
            "Step: 310 , train_loss: 676.9267578125, accuracy: 0.9375\n",
            "Step: 320 , train_loss: 801.098388671875, accuracy: 0.9296875\n",
            "Step: 330 , train_loss: 363.0784912109375, accuracy: 0.96875\n",
            "Step: 340 , train_loss: 641.1123046875, accuracy: 0.9453125\n",
            "Step: 350 , train_loss: 423.8111877441406, accuracy: 0.9375\n",
            "Step: 360 , train_loss: 911.8270263671875, accuracy: 0.9140625\n",
            "Step: 370 , train_loss: 1443.6806640625, accuracy: 0.9140625\n",
            "Step: 380 , train_loss: 864.7171630859375, accuracy: 0.9296875\n",
            "Step: 390 , train_loss: 959.5194091796875, accuracy: 0.9140625\n",
            "Step: 400 , train_loss: 472.4552307128906, accuracy: 0.953125\n",
            "Step: 410 , train_loss: 435.1499938964844, accuracy: 0.953125\n",
            "Step: 420 , train_loss: 801.1117553710938, accuracy: 0.9453125\n",
            "Step: 430 , train_loss: 326.3727722167969, accuracy: 0.9453125\n",
            "Step: 440 , train_loss: 932.649658203125, accuracy: 0.9375\n",
            "Step: 450 , train_loss: 260.809814453125, accuracy: 0.9375\n",
            "Step: 460 , train_loss: 324.7431640625, accuracy: 0.96875\n",
            "Step: 470 , train_loss: 387.2688903808594, accuracy: 0.9609375\n",
            "Step: 480 , train_loss: 308.30035400390625, accuracy: 0.9609375\n",
            "Step: 490 , train_loss: 129.47720336914062, accuracy: 0.96875\n",
            "Step: 500 , train_loss: 310.06756591796875, accuracy: 0.9609375\n",
            "Step: 510 , train_loss: 1031.89453125, accuracy: 0.9375\n",
            "Step: 520 , train_loss: 131.65972900390625, accuracy: 0.96875\n",
            "Step: 530 , train_loss: 894.0224609375, accuracy: 0.9296875\n",
            "Step: 540 , train_loss: 542.0886840820312, accuracy: 0.9453125\n",
            "Step: 550 , train_loss: 487.8897705078125, accuracy: 0.953125\n",
            "Step: 560 , train_loss: 469.47760009765625, accuracy: 0.9453125\n",
            "Step: 570 , train_loss: 234.62088012695312, accuracy: 0.9765625\n",
            "Step: 580 , train_loss: 183.5003662109375, accuracy: 0.9609375\n",
            "Step: 590 , train_loss: 372.000732421875, accuracy: 0.9609375\n",
            "Step: 600 , train_loss: 446.8426818847656, accuracy: 0.96875\n",
            "Step: 610 , train_loss: 238.11148071289062, accuracy: 0.96875\n",
            "Step: 620 , train_loss: 378.7662353515625, accuracy: 0.9375\n",
            "Step: 630 , train_loss: 273.4239501953125, accuracy: 0.9453125\n",
            "Step: 640 , train_loss: 77.855712890625, accuracy: 0.9765625\n",
            "Step: 650 , train_loss: 29.677032470703125, accuracy: 0.984375\n",
            "Step: 660 , train_loss: 384.36859130859375, accuracy: 0.953125\n",
            "Step: 670 , train_loss: 587.8921508789062, accuracy: 0.9375\n",
            "Step: 680 , train_loss: 232.4134521484375, accuracy: 0.9765625\n",
            "Step: 690 , train_loss: 264.4989013671875, accuracy: 0.9765625\n",
            "Step: 700 , train_loss: 190.96466064453125, accuracy: 0.96875\n",
            "Step: 710 , train_loss: 317.08447265625, accuracy: 0.9765625\n",
            "Step: 720 , train_loss: 180.1329345703125, accuracy: 0.9765625\n",
            "Step: 730 , train_loss: 187.66688537597656, accuracy: 0.96875\n",
            "Step: 740 , train_loss: 108.313720703125, accuracy: 0.9765625\n",
            "Step: 750 , train_loss: 115.92680358886719, accuracy: 0.9609375\n",
            "Step: 760 , train_loss: 28.065292358398438, accuracy: 0.9921875\n",
            "Step: 770 , train_loss: 178.09088134765625, accuracy: 0.984375\n",
            "Step: 780 , train_loss: 200.32080078125, accuracy: 0.96875\n",
            "Step: 790 , train_loss: 20.910446166992188, accuracy: 0.9921875\n",
            "Step: 800 , train_loss: 532.287109375, accuracy: 0.9296875\n",
            "Step: 810 , train_loss: 187.66683959960938, accuracy: 0.96875\n",
            "Step: 820 , train_loss: 88.78289794921875, accuracy: 0.9765625\n",
            "Step: 830 , train_loss: 347.1565856933594, accuracy: 0.9453125\n",
            "Step: 840 , train_loss: 97.2197265625, accuracy: 0.9765625\n",
            "Step: 850 , train_loss: 244.92788696289062, accuracy: 0.96875\n",
            "Step: 860 , train_loss: 267.1962890625, accuracy: 0.953125\n",
            "Step: 870 , train_loss: 61.539878845214844, accuracy: 0.984375\n",
            "Step: 880 , train_loss: 327.3385009765625, accuracy: 0.96875\n",
            "Step: 890 , train_loss: 165.81265258789062, accuracy: 0.9765625\n",
            "Step: 900 , train_loss: 700.2589111328125, accuracy: 0.953125\n",
            "Step: 910 , train_loss: 547.8208618164062, accuracy: 0.96875\n",
            "Step: 920 , train_loss: 306.5909118652344, accuracy: 0.9609375\n",
            "Step: 930 , train_loss: 211.48733520507812, accuracy: 0.984375\n",
            "Step: 940 , train_loss: 325.7706298828125, accuracy: 0.96875\n",
            "Step: 950 , train_loss: 187.30712890625, accuracy: 0.9765625\n",
            "Step: 960 , train_loss: 149.40206909179688, accuracy: 0.9765625\n",
            "Step: 970 , train_loss: 433.5487976074219, accuracy: 0.953125\n",
            "Step: 980 , train_loss: 107.90567016601562, accuracy: 0.953125\n",
            "Step: 990 , train_loss: 44.94850158691406, accuracy: 0.9921875\n",
            "Step: 1000 , train_loss: 153.45217895507812, accuracy: 0.984375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbL7VXHh7L4o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}