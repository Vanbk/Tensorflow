{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyObD/E0KLlBHUGLZSqsRsg6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vanbk/Tensorflow/blob/master/GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4u3I_L8qtFNa",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCbEjVHxrOqB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division, print_function, absolute_import\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5dBK2-krPgM",
        "colab_type": "code",
        "outputId": "15187ac9-a1d1-4369-93f3-0cfc4851cde6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Import MNIST data\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZnuWwlSsGQA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training params\n",
        "num_steps = 70000\n",
        "batch_size = 128\n",
        "learning_rate = 0.0002\n",
        "\n",
        "# Network params\n",
        "image_dim = 784\n",
        "gen_hidden_dim = 256\n",
        "disc_hidden_dim = 256\n",
        "noise_dim = 100 # noise data points\n",
        "\n",
        "# A custom initialization (see Xavier Glorot init)\n",
        "def glorot_init(shape):\n",
        "    return tf.random_normal(shape=shape, stddev=1. / tf.sqrt(shape[0] / 2.))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DegMZ17Uz9eH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Store layers weight & bias\n",
        "weights = {\n",
        "    'gen_hidden1': tf.Variable(glorot_init([noise_dim, gen_hidden_dim])),\n",
        "    'gen_out': tf.Variable(glorot_init([gen_hidden_dim, image_dim])),\n",
        "    'disc_hidden1': tf.Variable(glorot_init([image_dim, disc_hidden_dim])),\n",
        "    'disc_out': tf.Variable(glorot_init([disc_hidden_dim, 1])),\n",
        "}\n",
        "biases = {\n",
        "    'gen_hidden1': tf.Variable(tf.zeros([gen_hidden_dim])),\n",
        "    'gen_out': tf.Variable(tf.zeros([image_dim])),\n",
        "    'disc_hidden1': tf.Variable(tf.zeros([disc_hidden_dim])),\n",
        "    'disc_out': tf.Variable(tf.zeros([1])),\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGR3QhaK0Mbi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generator\n",
        "def generator(x):\n",
        "    hidden_layer = tf.matmul(x, weights['gen_hidden1'])\n",
        "    hidden_layer = tf.add(hidden_layer, biases['gen_hidden1'])\n",
        "    hidden_layer = tf.nn.relu(hidden_layer)\n",
        "    out_layer = tf.matmul(hidden_layer, weights['gen_out'])\n",
        "    out_layer = tf.add(out_layer, biases['gen_out'])\n",
        "    out_layer = tf.nn.sigmoid(out_layer)\n",
        "    return out_layer\n",
        "\n",
        "\n",
        "# Discriminator\n",
        "def discriminator(x):\n",
        "    hidden_layer = tf.matmul(x, weights['disc_hidden1'])\n",
        "    hidden_layer = tf.add(hidden_layer, biases['disc_hidden1'])\n",
        "    hidden_layer = tf.nn.relu(hidden_layer)\n",
        "    out_layer = tf.matmul(hidden_layer, weights['disc_out'])\n",
        "    out_layer = tf.add(out_layer, biases['disc_out'])\n",
        "    out_layer = tf.nn.sigmoid(out_layer)\n",
        "    return out_layer\n",
        "\n",
        "# Build Networks\n",
        "# Network Inputs\n",
        "gen_input = tf.placeholder(tf.float32, shape=[None, noise_dim], name='input_noise')\n",
        "disc_input = tf.placeholder(tf.float32, shape=[None, image_dim], name='disc_input')\n",
        "\n",
        "# Build Generator Network\n",
        "gen_sample = generator(gen_input)\n",
        "\n",
        "# Build 2 Discriminator Networks (one from noise input, one from generated samples)\n",
        "disc_real = discriminator(disc_input)\n",
        "disc_fake = discriminator(gen_sample)\n",
        "\n",
        "# Build Loss\n",
        "gen_loss  = -tf.reduce_mean(tf.log(disc_fake))\n",
        "disc_loss = -tf.reduce_mean(tf.log(disc_real) + tf.log(1. - disc_fake))\n",
        "\n",
        "\n",
        "\n",
        "# Build Optimizers\n",
        "optimizer_gen = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "optimizer_disc = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "# Training Variables for each optimizer\n",
        "# By default in TensorFlow, all variables are updated by each optimizer, so we\n",
        "# need to precise for each one of them the specific variables to update.\n",
        "# Generator Network Variables\n",
        "gen_vars = [weights['gen_hidden1'], weights['gen_out'],\n",
        "            biases['gen_hidden1'], biases['gen_out']]\n",
        "# Discriminator Network Variables\n",
        "disc_vars = [weights['disc_hidden1'], weights['disc_out'],\n",
        "            biases['disc_hidden1'], biases['disc_out']]\n",
        "\n",
        "# Create training operations\n",
        "train_gen = optimizer_gen.minimize(gen_loss, var_list=gen_vars)\n",
        "train_disc = optimizer_disc.minimize(disc_loss, var_list=disc_vars)\n",
        "\n",
        "# Initialize the variables (i.e. assign their default value)\n",
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNWsRFNf0Uf8",
        "colab_type": "code",
        "outputId": "c515a60a-89cf-4bdb-d1f4-256ba3011f1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Start Training\n",
        "# Start a new TF session\n",
        "sess = tf.Session()\n",
        "\n",
        "# Run the initializer\n",
        "sess.run(init)\n",
        "\n",
        "# Training\n",
        "for i in range(1, num_steps+1):\n",
        "    # Prepare Data\n",
        "    # Get the next batch of MNIST data (only images are needed, not labels)\n",
        "    batch_x, _ = mnist.train.next_batch(batch_size)\n",
        "    # Generate noise to feed to the generator\n",
        "    z = np.random.uniform(-1., 1., size=[batch_size, noise_dim])\n",
        "\n",
        "    # Train\n",
        "    feed_dict = {disc_input: batch_x, gen_input: z}\n",
        "    _, _, gl, dl = sess.run([train_gen, train_disc, gen_loss, disc_loss],\n",
        "                            feed_dict=feed_dict)\n",
        "    if i % 1000 == 0 or i == 1:\n",
        "        print('Step %i: Generator Loss: %f, Discriminator Loss: %f' % (i, gl, dl))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 1: Generator Loss: 0.875220, Discriminator Loss: 1.272933\n",
            "Step 1000: Generator Loss: 3.805916, Discriminator Loss: 0.060402\n",
            "Step 2000: Generator Loss: 4.851340, Discriminator Loss: 0.026262\n",
            "Step 3000: Generator Loss: 5.490080, Discriminator Loss: 0.011823\n",
            "Step 4000: Generator Loss: 3.412948, Discriminator Loss: 0.120643\n",
            "Step 5000: Generator Loss: 4.372851, Discriminator Loss: 0.049441\n",
            "Step 6000: Generator Loss: 4.435302, Discriminator Loss: 0.077358\n",
            "Step 7000: Generator Loss: 4.394002, Discriminator Loss: 0.077288\n",
            "Step 8000: Generator Loss: 4.490340, Discriminator Loss: 0.081114\n",
            "Step 9000: Generator Loss: 4.624085, Discriminator Loss: 0.085824\n",
            "Step 10000: Generator Loss: 4.222762, Discriminator Loss: 0.087292\n",
            "Step 11000: Generator Loss: 4.938044, Discriminator Loss: 0.094075\n",
            "Step 12000: Generator Loss: 4.013779, Discriminator Loss: 0.149057\n",
            "Step 13000: Generator Loss: 3.835727, Discriminator Loss: 0.210956\n",
            "Step 14000: Generator Loss: 4.264716, Discriminator Loss: 0.132385\n",
            "Step 15000: Generator Loss: 3.984204, Discriminator Loss: 0.192806\n",
            "Step 16000: Generator Loss: 4.251818, Discriminator Loss: 0.240226\n",
            "Step 17000: Generator Loss: 4.032470, Discriminator Loss: 0.294234\n",
            "Step 18000: Generator Loss: 4.732567, Discriminator Loss: 0.191744\n",
            "Step 19000: Generator Loss: 4.010635, Discriminator Loss: 0.290553\n",
            "Step 20000: Generator Loss: 4.430044, Discriminator Loss: 0.132616\n",
            "Step 21000: Generator Loss: 3.615584, Discriminator Loss: 0.247808\n",
            "Step 22000: Generator Loss: 3.662696, Discriminator Loss: 0.337171\n",
            "Step 23000: Generator Loss: 4.325129, Discriminator Loss: 0.256064\n",
            "Step 24000: Generator Loss: 3.833906, Discriminator Loss: 0.331278\n",
            "Step 25000: Generator Loss: 4.139482, Discriminator Loss: 0.287730\n",
            "Step 26000: Generator Loss: 3.633627, Discriminator Loss: 0.275105\n",
            "Step 27000: Generator Loss: 3.543438, Discriminator Loss: 0.236651\n",
            "Step 28000: Generator Loss: 3.508553, Discriminator Loss: 0.265635\n",
            "Step 29000: Generator Loss: 3.143611, Discriminator Loss: 0.365493\n",
            "Step 30000: Generator Loss: 3.149954, Discriminator Loss: 0.235508\n",
            "Step 31000: Generator Loss: 3.718683, Discriminator Loss: 0.198890\n",
            "Step 32000: Generator Loss: 3.446858, Discriminator Loss: 0.307990\n",
            "Step 33000: Generator Loss: 2.964002, Discriminator Loss: 0.257900\n",
            "Step 34000: Generator Loss: 3.070966, Discriminator Loss: 0.406099\n",
            "Step 35000: Generator Loss: 3.125353, Discriminator Loss: 0.345192\n",
            "Step 36000: Generator Loss: 3.121939, Discriminator Loss: 0.422390\n",
            "Step 37000: Generator Loss: 3.067072, Discriminator Loss: 0.401450\n",
            "Step 38000: Generator Loss: 2.911428, Discriminator Loss: 0.373220\n",
            "Step 39000: Generator Loss: 3.238099, Discriminator Loss: 0.370223\n",
            "Step 40000: Generator Loss: 3.054241, Discriminator Loss: 0.507491\n",
            "Step 41000: Generator Loss: 3.054134, Discriminator Loss: 0.448237\n",
            "Step 42000: Generator Loss: 3.055204, Discriminator Loss: 0.458136\n",
            "Step 43000: Generator Loss: 2.676888, Discriminator Loss: 0.561591\n",
            "Step 44000: Generator Loss: 2.531446, Discriminator Loss: 0.439644\n",
            "Step 45000: Generator Loss: 2.677298, Discriminator Loss: 0.564077\n",
            "Step 46000: Generator Loss: 2.586029, Discriminator Loss: 0.450522\n",
            "Step 47000: Generator Loss: 2.929001, Discriminator Loss: 0.439718\n",
            "Step 48000: Generator Loss: 2.830968, Discriminator Loss: 0.424747\n",
            "Step 49000: Generator Loss: 3.016073, Discriminator Loss: 0.626447\n",
            "Step 50000: Generator Loss: 2.500617, Discriminator Loss: 0.485050\n",
            "Step 51000: Generator Loss: 2.520818, Discriminator Loss: 0.521389\n",
            "Step 52000: Generator Loss: 2.769529, Discriminator Loss: 0.486496\n",
            "Step 53000: Generator Loss: 2.653332, Discriminator Loss: 0.421100\n",
            "Step 54000: Generator Loss: 3.072627, Discriminator Loss: 0.448746\n",
            "Step 55000: Generator Loss: 2.705135, Discriminator Loss: 0.494175\n",
            "Step 56000: Generator Loss: 2.662689, Discriminator Loss: 0.492398\n",
            "Step 57000: Generator Loss: 2.725589, Discriminator Loss: 0.483692\n",
            "Step 58000: Generator Loss: 2.823030, Discriminator Loss: 0.416689\n",
            "Step 59000: Generator Loss: 2.783979, Discriminator Loss: 0.449831\n",
            "Step 60000: Generator Loss: 2.710811, Discriminator Loss: 0.367541\n",
            "Step 61000: Generator Loss: 2.451145, Discriminator Loss: 0.528943\n",
            "Step 62000: Generator Loss: 2.744647, Discriminator Loss: 0.429592\n",
            "Step 63000: Generator Loss: 3.026646, Discriminator Loss: 0.386460\n",
            "Step 64000: Generator Loss: 2.868864, Discriminator Loss: 0.418406\n",
            "Step 65000: Generator Loss: 2.648891, Discriminator Loss: 0.405581\n",
            "Step 66000: Generator Loss: 3.094037, Discriminator Loss: 0.393983\n",
            "Step 67000: Generator Loss: 2.868654, Discriminator Loss: 0.446457\n",
            "Step 68000: Generator Loss: 2.689806, Discriminator Loss: 0.497228\n",
            "Step 69000: Generator Loss: 2.733470, Discriminator Loss: 0.422467\n",
            "Step 70000: Generator Loss: 2.738742, Discriminator Loss: 0.408560\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiC9F5aP0Zrp",
        "colab_type": "code",
        "outputId": "ea8b0cee-0f75-4165-f260-64d569dfea4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "# Testing\n",
        "# Generate images from noise, using the generator network.\n",
        "n = 6\n",
        "canvas = np.empty((28 * n, 28 * n))\n",
        "for i in range(n):\n",
        "    # Noise input.\n",
        "    z = np.random.uniform(-1., 1., size=[n, noise_dim])\n",
        "    # Generate image from noise.\n",
        "    g = sess.run(gen_sample, feed_dict={gen_input: z})\n",
        "    # Reverse colours for better display\n",
        "    g = -1 * (g - 1)\n",
        "    for j in range(n):\n",
        "        # Draw the generated digits\n",
        "        canvas[i * 28:(i + 1) * 28, j * 28:(j + 1) * 28] = g[j].reshape([28, 28])\n",
        "\n",
        "plt.figure(figsize=(n, n))\n",
        "plt.imshow(canvas, origin=\"upper\", cmap=\"gray\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4d35659e710b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcanvas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Noise input.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqD2Pzmn4U7t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}